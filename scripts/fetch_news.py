#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç§‘æŠ€åœˆæ¯æ—¥æ–°é—»æŠ“å–è„šæœ¬
ä»å¤šä¸ªç§‘æŠ€æ–°é—»æºæŠ“å–æœ€æ–°èµ„è®¯
"""

import json
import os
import re
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path

import feedparser
import requests
from bs4 import BeautifulSoup

# åŒ—äº¬æ—¶é—´ UTC+8
CST = timezone(timedelta(hours=8))

# æ–°é—»æºé…ç½®
NEWS_SOURCES = [
    {
        "name": "Hacker News",
        "url": "https://news.ycombinator.com/rss",
        "type": "rss",
        "category": "ç»¼åˆç§‘æŠ€",
        "icon": "ğŸ”¶"
    },
    {
        "name": "The Verge",
        "url": "https://www.theverge.com/rss/index.xml",
        "type": "rss",
        "category": "ç§‘æŠ€èµ„è®¯",
        "icon": "ğŸ“±"
    },
    {
        "name": "TechCrunch",
        "url": "https://techcrunch.com/feed/",
        "type": "rss",
        "category": "åˆ›ä¸šç§‘æŠ€",
        "icon": "ğŸš€"
    },
    {
        "name": "Wired",
        "url": "https://www.wired.com/feed/rss",
        "type": "rss",
        "category": "ç§‘æŠ€æ–‡åŒ–",
        "icon": "âš¡"
    },
    {
        "name": "MIT Technology Review",
        "url": "https://www.technologyreview.com/feed/",
        "type": "rss",
        "category": "å‰æ²¿æŠ€æœ¯",
        "icon": "ğŸ”¬"
    },
    {
        "name": "Ars Technica",
        "url": "https://feeds.arstechnica.com/arstechnica/index",
        "type": "rss",
        "category": "æ·±åº¦ç§‘æŠ€",
        "icon": "ğŸ–¥ï¸"
    },
    {
        "name": "36æ°ª",
        "url": "https://36kr.com/feed",
        "type": "rss",
        "category": "å›½å†…ç§‘æŠ€",
        "icon": "ğŸ‡¨ğŸ‡³"
    },
    {
        "name": "å°‘æ•°æ´¾",
        "url": "https://sspai.com/feed",
        "type": "rss",
        "category": "æ•°å­—ç”Ÿæ´»",
        "icon": "ğŸ“²"
    },
]

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}


def clean_html(text: str) -> str:
    """æ¸…ç† HTML æ ‡ç­¾"""
    if not text:
        return ""
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ", strip=True)[:300]


def fetch_rss(source: dict) -> list:
    """æŠ“å– RSS æº"""
    articles = []
    try:
        feed = feedparser.parse(source["url"])
        for entry in feed.entries[:8]:
            title = entry.get("title", "").strip()
            link = entry.get("link", "").strip()
            summary = clean_html(
                entry.get("summary", entry.get("description", ""))
            )
            published = entry.get("published", entry.get("updated", ""))

            if title and link:
                articles.append({
                    "title": title,
                    "url": link,
                    "summary": summary,
                    "source": source["name"],
                    "category": source["category"],
                    "icon": source["icon"],
                    "published": published,
                })
    except Exception as e:
        print(f"[WARN] æŠ“å– {source['name']} å¤±è´¥: {e}")
    return articles


def fetch_all_news() -> list:
    """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
    all_articles = []
    for source in NEWS_SOURCES:
        print(f"  â†’ æŠ“å– {source['name']} ...")
        articles = fetch_rss(source)
        all_articles.extend(articles)
        time.sleep(1)
    return all_articles


def save_news(articles: list, output_dir: Path):
    """ä¿å­˜æ–°é—»æ•°æ®"""
    output_dir.mkdir(parents=True, exist_ok=True)
    now = datetime.now(CST)
    date_str = now.strftime("%Y-%m-%d")

    data = {
        "date": date_str,
        "generated_at": now.isoformat(),
        "total": len(articles),
        "articles": articles,
    }

    # ä¿å­˜å½“æ—¥æ•°æ®
    daily_file = output_dir / f"{date_str}.json"
    with open(daily_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # æ›´æ–° latest.json
    latest_file = output_dir / "latest.json"
    with open(latest_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # æ›´æ–°ç´¢å¼•
    update_index(output_dir, date_str)

    print(f"âœ… å·²ä¿å­˜ {len(articles)} æ¡æ–°é—»åˆ° {daily_file}")
    return date_str


def update_index(output_dir: Path, new_date: str):
    """æ›´æ–°æ—¥æœŸç´¢å¼•æ–‡ä»¶"""
    index_file = output_dir / "index.json"
    dates = []

    if index_file.exists():
        with open(index_file, "r", encoding="utf-8") as f:
            dates = json.load(f).get("dates", [])

    if new_date not in dates:
        dates.insert(0, new_date)
        dates = dates[:30]  # ä¿ç•™æœ€è¿‘30å¤©

    with open(index_file, "w", encoding="utf-8") as f:
        json.dump({"dates": dates}, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æŠ“å–ç§‘æŠ€æ–°é—»...")
    data_dir = Path("data/news")
    articles = fetch_all_news()
    date_str = save_news(articles, data_dir)
    print(f"ğŸ“… {date_str} æ–°é—»æŠ“å–å®Œæˆï¼Œå…± {len(articles)} æ¡")
